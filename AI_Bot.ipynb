{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shaz-gif/tiny-jarvis/blob/main/AI_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#AI Chatbot Assignment\n",
        "### Apport Software Solutions Private Limited\n",
        "### Role :  AI Engineer\n",
        "Name : Shashwat Raj \\\\\n",
        "Entry Number: 2021MT10259 \\\\\n",
        "College: IIT Delhi \\\\\n",
        "**Please open this note book in colab using the following sharing like, because this contains images and links for documentation that will be better displayed on Colab** \\\\\n",
        "Link to this colab notebook: - [Shashwat's Colab](https://colab.research.google.com/drive/13W-X7MPBSBww1eNnhCHS1x5U6yqptFPi?usp=sharing)"
      ],
      "metadata": {
        "id": "fg44lvlmptvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Important Note\n",
        "\n",
        "Due to the numerous dependencies, the trial-and-error nature of the development process, and the need to document the code alongside implementation, I decided to use Google Colab for this project. This setup allows for an efficient and organized workflow.\n",
        "\n",
        "I want to clarify that the code has not been uploaded anywhere on the internet and is only stored privately in my Colab account. It is accessible solely through a private link, ensuring adherence to proper conduct codes and data privacy practices.\n"
      ],
      "metadata": {
        "id": "jedG9kkSzTDJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izNeUi--hSSA"
      },
      "source": [
        "\n",
        "# **Chatbot for PDF-Based Q&A**\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "Create a chatbot that can answer questions based on the content of a provided PDF document. If the chatbot cannot find the answer in the PDF, it should respond:\n",
        "\n",
        "\"Sorry, I didn’t understand your question. Do you want to connect with a live agent?\"\n",
        "\n",
        "---\n",
        "\n",
        "## **Assignment Requirements**\n",
        "\n",
        "### 1. **PDF Understanding**\n",
        "- The chatbot should load the provided PDF and use its content as the knowledge base.\n",
        "- It must accurately retrieve and display answers from the PDF content.\n",
        "\n",
        "### 2. **Fallback Response**\n",
        "- If the chatbot cannot find the answer in the PDF, it must provide a fallback response:\n",
        "  \"Sorry, I didn’t understand your question. Do you want to connect with a live agent?\"\n",
        "\n",
        "### 3. **User Interaction**\n",
        "- Develop an interface (text-based or graphical) where users can ask questions and receive responses.\n",
        "- Ensure the interaction is clear and intuitive.\n",
        "\n",
        "### 4. **Problem-Solving**\n",
        "- You are responsible for deciding the tools, libraries, and frameworks to use.\n",
        "- Research and implement methods for handling PDF data, querying information, and building chatbot functionality.\n",
        "\n",
        "---\n",
        "\n",
        "## **Guidelines**\n",
        "\n",
        "- Focus on applying AI concepts and practical problem-solving skills.\n",
        "- Ensure your solution is modular and maintainable.\n",
        "- Document your approach, including the decisions made and challenges encountered.\n",
        "\n",
        "---\n",
        "\n",
        "## **Deliverables**\n",
        "\n",
        "### 1. **Chatbot Application**\n",
        "- A functional chatbot that meets the requirements.\n",
        "\n",
        "### 2. **Documentation**\n",
        "- Describe your approach to solving the problem.\n",
        "- Include details on tools and techniques used, and how the fallback logic is implemented.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI76-BMFhUmS"
      },
      "source": [
        "![Piping and Instrumentation Diagram](https://drive.google.com/uc?id=15AosHDCUudfYp21mDChKeEELV_zUgKvW)\n",
        "\n",
        "*Image made by Shashwat: [Canva Link](https://www.canva.com/design/DAGX6P98jv0/_hkeMRWL92X_AQCRPkP76Q/edit?utm_content=DAGX6P98jv0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Piping and Instrumentation Diagram (P&ID): End-to-End Design of AI Bot**\n",
        "\n",
        "Above diagram illustrates the architecture and workflow for an AI chatbot designed to answer questions based on documents like PDFs, DOCs, and TXT files. Here's an explanation of the components:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Data Source**\n",
        "- **Input Formats:** The user uploads files in supported formats:\n",
        "  - **PDF** : Implemented\n",
        "  - **DOC** : Can be extended to\n",
        "  - **TXT** : Can be extended to\n",
        "- **Purpose:** These files act as the knowledge base for the chatbot.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Data Parsing**\n",
        "- **Chunking:**\n",
        "  - The uploaded documents are broken down into smaller, manageable chunks (e.g., `chunk01`, `chunk02`, etc.).\n",
        "  - **Reason:** This helps in processing and embedding the text more efficiently for querying.\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Embedding Models**\n",
        "- **Purpose:** Convert the text chunks into vector embeddings, which are mathematical representations of the text.\n",
        "- **Process:**\n",
        "  - Each chunk is processed using a pre-trained **embedding model**. In my small project, I used SentenceTransformer\n",
        "\\*[Sentence Transformer Documentation](https://www.canva.com/design/DAGX6P98jv0/_hkeMRWL92X_AQCRPkP76Q/edit?utm_content=DAGX6P98jv0&utm_campaign=designshare&utm_medium=link2&utm_source=sharebutton)*\n",
        "  - Outputs are **vector embeddings**, which capture the semantic meaning of the text.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Data Warehouse (Storage)**\n",
        "- **Embedded Text Corpus:**\n",
        "  - The embeddings are stored in a database for efficient retrieval.\n",
        "  - Example: For PDF1, embeddings might include sentences like \"Approval of class...\".\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Vector Database**\n",
        "- **Examples:** Tools like **Pinecone** or **SingleStore** are used. I have used **Pinecone**.\n",
        "- **Functionality:**\n",
        "  - The vector embeddings from the embedding model are stored in this database.\n",
        "  - This allows for fast similarity searches when a user query is processed.\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Query Processing**\n",
        "- **User Query:**\n",
        "  - The user inputs a query (e.g., \"What is date of exam ...?\").\n",
        "  - The query is embedded into a vector format using the same embedding model.\n",
        "- **Cosine Similarity:**\n",
        "  - The vectorized query is compared with the stored embeddings using cosine similarity to determine relevance.\n",
        "\n",
        "---\n",
        "\n",
        "## **7. Results**\n",
        "- **Similarity Scores:**\n",
        "  - Result tokens are ranked based on how closely they match the query (e.g., token1: 0.98, token4: 0.79, etc.).\n",
        "  - These results are presented to the user in descending order of similarity.\n",
        "\n",
        "---\n",
        "\n",
        "## **8. User Interface**\n",
        "- **Purpose:** Provides an intuitive platform for user interaction.\n",
        "- **Features:**\n",
        "  - Users can input queries and view responses.\n",
        "  - Can be extended to more sophasticated user interactions and UI\n",
        "---\n",
        "\n",
        "## **Backend (Highlighted in the Diagram)**\n",
        "- The backend handles:\n",
        "  - Parsing and embedding the uploaded documents.\n",
        "  - Storing embeddings in a vector database.\n",
        "  - Processing user queries and retrieving results based on similarity.\n",
        "\n",
        "---\n",
        "\n",
        "This architecture ensures efficient and accurate question-answering while maintaining user-friendly interaction through fallback mechanisms and feedback loops.\n"
      ],
      "metadata": {
        "id": "Ww0xFoXgrZzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PDF Text Extraction Functionality\n",
        "\n",
        "\n",
        "This code enables PDF file upload and text extraction using `ipywidgets` and `PyPDF2`.\n",
        "\n",
        "1. **`extract_text_from_pdf(pdf_path)`**:\n",
        "   - This function accepts a file path (`pdf_path`), opens the PDF file, and extracts text from all of its pages.\n",
        "   - It uses `PyPDF2.PdfReader` to read the PDF and the `extract_text()` method to retrieve text content.\n",
        "   - Returns the extracted text or `None` if an error occurs.\n",
        "\n",
        "2. **`on_upload_change(change)`**:\n",
        "   - This is a callback function that gets triggered when a file is uploaded using the `widgets.FileUpload`.\n",
        "   - It extracts the uploaded file, saves it as `uploaded_pdf.pdf` in the local path `/content/`, and prints a success message.\n",
        "   - Then, it calls `extract_text_from_pdf()` to extract text from the saved PDF and displays the result or an error message.\n",
        "\n",
        "3. **File Upload Widget (`uploader`)**:\n",
        "   - `widgets.FileUpload` is used to create a file upload interface that accepts only `.pdf` files and allows a single file to be uploaded.\n",
        "   - The `observe()` method listens for file uploads and calls `on_upload_change()` when the file is uploaded.\n",
        "\n",
        "4. **File Upload Process**:\n",
        "   - The widget allows the user to upload a PDF file. After the file is uploaded, it triggers the callback to save the file and extract the text.\n",
        "\n",
        "This code provides an interactive way to upload and extract text from a PDF file in a Jupyter notebook environment.\n",
        "\n"
      ],
      "metadata": {
        "id": "XzqmGifI_OwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "\n",
        "pdf_text = None"
      ],
      "metadata": {
        "id": "rR5mH4cnxVYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"\n",
        "    Extracts text from a PDF file.\n",
        "    Args:\n",
        "        pdf_path (str): Path to the PDF file.\n",
        "    Returns:\n",
        "        str: Combined text from all pages of the PDF.\n",
        "    \"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            pdf_reader = PyPDF2.PdfReader(file)\n",
        "            for page in pdf_reader.pages:\n",
        "                text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error while reading PDF: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "Z-ZMhX9lx2o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def on_upload_change(change):\n",
        "    \"\"\"\n",
        "    Callback function that is triggered when a file is uploaded.\n",
        "    \"\"\"\n",
        "    global pdf_text\n",
        "    uploaded_file = next(iter(uploader.value.values()))\n",
        "\n",
        "    file_path = \"/content/uploaded_pdf.pdf\"\n",
        "    with open(file_path, 'wb') as f:\n",
        "        f.write(uploaded_file['content'])\n",
        "\n",
        "    print(f\"File uploaded successfully: {file_path}\")\n",
        "\n",
        "    pdf_text = extract_text_from_pdf(file_path)\n",
        "    if pdf_text:\n",
        "        print(f\"Extracted {len(pdf_text)} characters from the PDF.\")\n",
        "    else:\n",
        "        print(\"Failed to extract text.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "YzMj_MsDx4Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploader = widgets.FileUpload(\n",
        "    accept='.pdf',  # Only accept PDF files\n",
        "    multiple=False  # Only allow a single file to be uploaded\n",
        ")\n",
        "\n",
        "\n",
        "uploader.observe(on_upload_change, names='value')\n",
        "\n",
        "display(uploader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "0ac59b138acc49629a97d83cbad9b9f5",
            "74207a5c63644fc4a2dda0ae5fc47bd6",
            "269d975266a3420daafc85b97d1f2dcd"
          ]
        },
        "id": "8ym8kQhsx7V5",
        "outputId": "8b9d7d7e-8a03-48f0-8e39-83a55ee301dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "FileUpload(value={}, accept='.pdf', description='Upload')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ac59b138acc49629a97d83cbad9b9f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File uploaded successfully: /content/uploaded_pdf.pdf\n",
            "Extracted 94788 characters from the PDF.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing the Extracted Text\n",
        "### Function 1: `split_text_into_chunks`\n",
        "\n",
        "This function splits a given text into smaller chunks to optimize it for processing by embedding models.\n",
        "\n",
        "- **Arguments:**\n",
        "  - `text` (str): The input text to be split.\n",
        "  - `chunk_size` (int, default=500): The maximum size of each chunk in characters.\n",
        "\n",
        "- **Returns:**\n",
        "  - A list of text chunks, where each chunk is a substring of the input text with a size no greater than `chunk_size`.\n",
        "\n",
        "- **Explanation:**\n",
        "  - The function iterates over the words in the input text and builds chunks. When the current chunk exceeds the `chunk_size`, the chunk is added to the list and a new chunk begins.\n",
        "  - After processing all words, any remaining text in `current_chunk` is added as the final chunk.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZcgseQQj_xLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_text_into_chunks(text, chunk_size=500):\n",
        "    \"\"\"\n",
        "    Splits the text into smaller chunks.\n",
        "    Args:\n",
        "        text (str): The input text.\n",
        "        chunk_size (int): Maximum size of each chunk in characters.\n",
        "    Returns:\n",
        "        list: List of text chunks.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for word in text.split():\n",
        "        if len(current_chunk) + len(word) + 1 > chunk_size:\n",
        "            chunks.append(current_chunk.strip())\n",
        "            current_chunk = word\n",
        "        else:\n",
        "            current_chunk += \" \" + word\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "a4n7ZWcZ_zar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks = split_text_into_chunks(pdf_text, chunk_size=500)\n",
        "\n",
        "print(f\"Created {len(text_chunks)} text chunks.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "St47JQrWAGwC",
        "outputId": "60acc69b-09b2-4976-f2d7-340272c1bc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 191 text chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXzzYE0hhU64"
      },
      "source": [
        "## Errors and Issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNk3hfyAtRNj"
      },
      "source": [
        "### Embedding Generation Error\n",
        "\n",
        "While generating embeddings using OpenAI's `text-embedding-ada-002` model, an error occurred with the message:\n",
        "\n",
        "\n",
        "This error indicates that the usage quota for the current OpenAI plan was exceeded. As a result, to proceed with generating embeddings, a different, free model will be used as an alternative.\n",
        "\n",
        "For more details on quota limits and billing, refer to the [OpenAI API error codes documentation](https://platform.openai.com/docs/guides/error-codes/api-errors).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative: Using Sentence Transformers for Embeddings\n",
        "\n",
        "Due to the error encountered with OpenAI's `text-embedding-ada-002` model (quota exceeded), I opted to use the **Sentence Transformer** model as an alternative. While Sentence Transformers may be slightly less accurate or efficient in some cases compared to OpenAI's embeddings, it is a great free alternative for generating embeddings from text.\n",
        "\n",
        "The Sentence Transformer model can generate high-quality embeddings and is widely used for tasks like semantic search, clustering, and text similarity analysis.\n",
        "\n",
        "By switching to this model, we can continue with the embedding generation process without needing a paid OpenAI plan.\n",
        "\n"
      ],
      "metadata": {
        "id": "X3u18UME2QOt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceC8kqMX-0z7"
      },
      "outputs": [],
      "source": [
        "# !pip install -qU \\\n",
        "#     pinecone-client==3.0.2 \\\n",
        "#     openai==1.10.0 \\\n",
        "#     datasets==2.16.1\n",
        "\n",
        "# from openai import OpenAI\n",
        "\n",
        "# client = OpenAI(\n",
        "#     api_key=\"******\"\n",
        "# )\n",
        "\n",
        "\n",
        "# !pip install openai --upgrade  # Ensure you have the latest OpenAI package installed\n",
        "\n",
        "# import openai\n",
        "\n",
        "# openai.api_key = \"******\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import openai\n",
        "\n",
        "# def generate_embeddings_v2(text_chunks):\n",
        "#     \"\"\"\n",
        "#     Generates embeddings for a list of text chunks using OpenAI's text-embedding-ada-002 model.\n",
        "#     Args:\n",
        "#         text_chunks (list): List of text chunks to embed.\n",
        "#     Returns:\n",
        "#         list: List of embeddings (one for each text chunk).\n",
        "#     \"\"\"\n",
        "#     embeddings = []\n",
        "#     try:\n",
        "#         # Use the new interface to batch process embeddings\n",
        "#         response = openai.Embedding.create(\n",
        "#             model=\"text-embedding-ada-002\",\n",
        "#             input=text_chunks  # Pass the entire list of text chunks at once\n",
        "#         )\n",
        "#         embeddings = [item['embedding'] for item in response['data']]  # Extract embeddings\n",
        "#     except Exception as e:\n",
        "#         print(f\"Error generating embeddings: {e}\")\n",
        "#     return embeddings\n"
      ],
      "metadata": {
        "id": "gtGw42tHGon3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # Generate embeddings for the text chunks\n",
        "# if pdf_text and 'text_chunks' in locals():\n",
        "#     embeddings = generate_embeddings_v2(text_chunks)\n",
        "#     if embeddings:\n",
        "#         print(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
        "# else:\n",
        "#     print(\"No text chunks to process!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtuIT2_gGrJs",
        "outputId": "c37d6a0e-18dd-4d62-97f8-b1f157c8808c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error generating embeddings: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Sentence-transformer"
      ],
      "metadata": {
        "id": "T6NFi1RH2fCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative: Using Sentence Transformers for Embeddings\n",
        "\n",
        "Due to the error encountered with OpenAI's `text-embedding-ada-002` model (quota exceeded), I opted to use the **Sentence Transformer** model as an alternative. While Sentence Transformers may be slightly less accurate or efficient in some cases compared to OpenAI's embeddings, it is a great free alternative for generating embeddings from text.\n",
        "\n",
        "#### Sentence Transformer Architecture\n",
        "\n",
        "Sentence Transformers are built on top of transformer-based models (such as BERT, RoBERTa, or DistilBERT) and are specifically designed for tasks that require sentence or text embeddings. The model generates dense vector representations of sentences that capture their semantic meaning, making them ideal for tasks like semantic search, clustering, and text similarity analysis. The architecture uses techniques like **Siamese networks** and **triplet loss** to train the model on sentence pairs, optimizing it for producing meaningful sentence-level embeddings.\n",
        "\n",
        "![Sentence Transformer Architecture](https://www.researchgate.net/profile/Mohamed-Gaber-2/publication/353487642/figure/fig2/AS:1050256590503937@1627412092943/Siamese-Sentence-Transformer-STransformer-Architecture.ppm)\n",
        "\n",
        " [Sentence Transformers](https://sbert.net/).\n",
        "\n",
        "By switching to this model, we can continue with the embedding generation process without needing a paid OpenAI plan.\n",
        "\n"
      ],
      "metadata": {
        "id": "2BwPPfRz3GA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmkALb_rIPRC",
        "outputId": "2d8f9985-4bd0-4c7d-8f9f-cb9672146727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.46.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.6)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.26.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.20.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n"
      ],
      "metadata": {
        "id": "MK75GADZIQjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Function: `generate_embeddings`\n",
        "\n",
        "This function generates embeddings for a list of text chunks using the **SentenceTransformers** model.\n",
        "\n",
        "- **Arguments:**\n",
        "  - `text_chunks` (list): A list of text chunks that need to be embedded.\n",
        "\n",
        "- **Returns:**\n",
        "  - A list of embeddings corresponding to each text chunk, where each embedding is a dense vector representation of the chunk's semantic meaning.\n",
        "\n",
        "- **Explanation:**\n",
        "  - The function uses the `encode` method from the SentenceTransformers model to generate embeddings for all the text chunks at once.\n",
        "  - The `show_progress_bar=True` argument displays a progress bar during the embedding generation process, especially useful when processing large datasets.\n"
      ],
      "metadata": {
        "id": "OLdMkIDn31sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_embeddings(text_chunks):\n",
        "    \"\"\"\n",
        "    Generate embeddings for a list of text chunks using SentenceTransformers.\n",
        "    Args:\n",
        "        text_chunks (list): List of text chunks to embed.\n",
        "    Returns:\n",
        "        list: List of embeddings for the text chunks.\n",
        "    \"\"\"\n",
        "    embeddings = model.encode(text_chunks, show_progress_bar=True)\n",
        "    return embeddings\n"
      ],
      "metadata": {
        "id": "_6WhWVmw3tPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for the text chunks\n",
        "if pdf_text and 'text_chunks' in locals():\n",
        "    embeddings = generate_embeddings(text_chunks)\n",
        "    print(f\"Generated embeddings for {len(embeddings)} chunks.\")\n",
        "else:\n",
        "    print(\"No text chunks to process!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "66d6bcb1fbb841c68123010f191cb7b6",
            "dc8a4cda10c54953be061c805b25b310",
            "fdd6beac6c97472c98b3fad10d4d17f5",
            "432837570eba48c380e2ca147e732488",
            "f7bd204320f04fed971431627fe424de",
            "efd7e1fb0cfc4a368a14c257c46c374a",
            "52da5e6c579944b4b3f06e4aab8b788b",
            "32e0bac87f06454a824e4bbae6f008de",
            "9b889b76618a42d6b944e738bc8f4c1c",
            "311f1a5011e442da802574706b1fd973",
            "a4d08f06438c4d6389096e9d3dfdab53"
          ]
        },
        "id": "_8G51r6q373d",
        "outputId": "6c2f251c-d07c-41f7-8eda-38ace4817dcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/6 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66d6bcb1fbb841c68123010f191cb7b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated embeddings for 191 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kbfItcn-00B"
      },
      "source": [
        "### Setting Up Pinecone for Storing Embeddings\n",
        "\n",
        "Pinecone is a fully managed vector database designed for similarity search and machine learning applications. It allows you to store, index, and search vector embeddings at scale. Pinecone makes it easy to manage and query large collections of high-dimensional vectors, such as embeddings generated from text or images, without needing to manage the underlying infrastructure.\n",
        "\n",
        "#### What is Pinecone?\n",
        "\n",
        "Pinecone is a cloud-native vector database that provides a high-performance platform to store, search, and retrieve vector embeddings. It is optimized for applications such as:\n",
        "\n",
        "- **Semantic Search**: Quickly retrieving relevant documents based on the similarity of their embeddings.\n",
        "- **Recommendation Systems**: Finding similar items (e.g., products, movies) based on user preferences or item characteristics.\n",
        "- **Anomaly Detection**: Identifying outliers by comparing vectors in a database.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![Sentence Transformer Architecture](https://miro.medium.com/v2/resize:fit:1200/1*4Z90ZgAq7nDdkuWe1bWnlg.jpeg)\n",
        "\n",
        " [Vector Databases](https://www.pinecone.io/learn/vector-database/)."
      ],
      "metadata": {
        "id": "rYNbUxQ07L_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### How Pinecone Works\n",
        "\n",
        "1. **Embedding Generation**: First, you generate vector embeddings from your text or data using machine learning models like Sentence Transformers or OpenAI’s embeddings.\n",
        "   \n",
        "2. **Storing Vectors**: Once you have the embeddings, you store them in Pinecone's vector database. Each vector is typically stored with metadata, such as IDs or other related information.\n",
        "\n",
        "3. **Similarity Search**: When you query the database, Pinecone computes the similarity between your input vector and the stored vectors using efficient nearest-neighbor search algorithms (like cosine similarity or Euclidean distance).\n",
        "\n",
        "4. **Real-Time Updates**: Pinecone supports real-time updates, allowing you to add, delete, or modify vectors dynamically as new data comes in.\n",
        "\n",
        "#### Why Use Pinecone?\n",
        "\n",
        "- **Scalability**: Pinecone can handle billions of vectors and ensures fast and reliable similarity searches even as your data grows.\n",
        "- **Managed Service**: It abstracts away the complexity of managing vector databases, letting you focus on building applications rather than maintaining infrastructure.\n",
        "- **Efficiency**: Pinecone offers highly optimized and low-latency search, making it suitable for real-time applications.\n",
        "\n",
        "By using Pinecone to store and index your embeddings, you can easily scale up your search and retrieval tasks, making it an essential tool for any application that involves similarity search or machine learning.\n"
      ],
      "metadata": {
        "id": "yWW-wMsV7KqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pinecone-client\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XrFFChcKJWK",
        "outputId": "56d24f79-da64-47fa-aa98-1e377afad99b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pinecone-client in /usr/local/lib/python3.10/dist-packages (3.0.2)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2024.8.30)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client) (2.2.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting Up Pinecone Index\n",
        "\n",
        "This code sets up a **Pinecone** vector database to store embeddings.\n",
        "\n",
        "1. **Initialize Pinecone**: The `Pinecone` client is initialized using an API key to authenticate the connection.\n",
        "\n",
        "2. **Check and Create Index**:\n",
        "   - It checks if the index `pdf-chatbot-index` exists.\n",
        "   - If not, it creates a new index with a dimension of `384` (suitable for **all-MiniLM-L6-v2** embeddings), using **cosine similarity** as the metric.\n",
        "\n",
        "3. **Connect to Index**: The code then connects to the specified index for future operations.\n",
        "\n",
        "This setup enables efficient similarity search using Pinecone’s managed vector database.\n"
      ],
      "metadata": {
        "id": "so6L78Tl8_r7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWM7tatP-00B"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "import os\n",
        "\n",
        "# Initialize Pinecone with your API key\n",
        "pc = Pinecone(api_key=\"*********\")\n",
        "\n",
        "# Replace with your API  , I have removed mine API Keys. This notebook will give error here if\n",
        "#run without providing API Keys.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pc.delete_index(index_name)\n",
        "\n",
        "# print(f\"Deleted the index: {index_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl9uC73AAq14",
        "outputId": "80a6d408-e508-4802-bb19-4f8199d936d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted the index: pdf-chatbot-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = \"pdf-chatbot-index\"\n",
        "\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    \"\"\"\n",
        "    Create a new index (adjust dimension to match the embedding dimension)\n",
        "    \"\"\"\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=384,                                                            # for all-MiniLM-L6-v2 embeddings\n",
        "        metric=\"cosine\",                                                          # Use cosine similarity for text embeddings\n",
        "        spec=ServerlessSpec(cloud='aws', region='us-east-1')\n",
        "    )\n"
      ],
      "metadata": {
        "id": "wlXpPq8QLxFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Connect to the Pinecone index\n",
        "index = pc.Index(index_name)\n",
        "\n",
        "print(f\"Connected to Pinecone index: {index_name}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I06q50yMfmq",
        "outputId": "b793aa6f-d5bf-4657-d3c4-2244534ac402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Pinecone index: pdf-chatbot-index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upserting Embeddings into Pinecone\n",
        "\n",
        "1. **Generate Unique IDs & Prepare Data**: Unique IDs are created for each text chunk, and embeddings are paired with these IDs and metadata (the original text) in preparation for insertion into Pinecone.\n",
        "\n",
        "2. **Upsert Data**: The `upsert()` method inserts the embeddings, IDs, and metadata into the Pinecone index, enabling efficient similarity search.\n"
      ],
      "metadata": {
        "id": "_ao2DRWQKeHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate unique IDs for the text chunks\n",
        "ids = [f\"chunk-{i}\" for i in range(len(embeddings))]\n",
        "\n",
        "vectors = [(ids[i], embeddings[i], {\"text\": text_chunks[i]}) for i in range(len(embeddings))]\n",
        "index.upsert(vectors)\n",
        "\n",
        "print(f\"Upserted {len(vectors)} embeddings into the Pinecone index.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UrVznMUrKenk",
        "outputId": "d71b24e1-c2b8-4e11-d925-6c2ea98d7ade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upserted 191 embeddings into the Pinecone index.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Querying Pinecone for Similar Chunks\n",
        "\n",
        "1. **Generate Query Embedding**:\n",
        "   - The user's question (`query_text`) is converted into an embedding using the Sentence Transformers model.\n",
        "\n",
        "2. **Query Pinecone**:\n",
        "   - The query embedding is used to search the Pinecone index, retrieving the top `k` most similar results. The results include metadata (text chunks) along with their similarity scores.\n",
        "\n",
        "3. **Filter Results by Similarity**:\n",
        "   - Results are filtered based on a **similarity threshold**. Only matches with scores above the threshold are considered valid and returned.\n",
        "\n"
      ],
      "metadata": {
        "id": "8baNywYgNCOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query_pinecone(query_text, top_k=5, similarity_threshold=0.4):\n",
        "    \"\"\"\n",
        "    Query the Pinecone index with a user question and retrieve the most similar chunks.\n",
        "    Args:\n",
        "        query_text (str): The user's question.\n",
        "        top_k (int): Number of top results to retrieve.\n",
        "        similarity_threshold (float): Minimum similarity score to consider a valid match.\n",
        "    Returns:\n",
        "        list: Matching results with scores above the threshold.\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode([query_text])[0]\n",
        "    query_embedding_list = query_embedding.tolist()\n",
        "\n",
        "    # Query Pinecone for the top-k most similar results\n",
        "    results = index.query(vector=query_embedding_list, top_k=top_k, include_metadata=True)\n",
        "\n",
        "    # Filter results based on the similarity threshold\n",
        "    matches = [\n",
        "        match for match in results[\"matches\"] if match[\"score\"] >= similarity_threshold\n",
        "    ]\n",
        "\n",
        "    return matches\n"
      ],
      "metadata": {
        "id": "Vp0pBTpLNBvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def user_query(user_question):\n",
        "    results = query_pinecone(user_question)\n",
        "\n",
        "    if results:\n",
        "        print(f\"Top matches: {results}\")\n",
        "    else:\n",
        "        print(\"Sorry, I didn’t understand your question. Do you want to connect with a live agent?\")\n",
        "\n",
        "    for match in results:\n",
        "      print(f\"{match['score']:.2f}: {match['metadata']['text']}\")"
      ],
      "metadata": {
        "id": "UvOto8PGA_LF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Some Example Usuage"
      ],
      "metadata": {
        "id": "aFLra_k7_Gqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are likely to be in the uploaded pdf\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "user_question = \"What are the consequences of Ragging & Sexual Harassment?\"\n",
        "user_query(user_question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEMba6yzN7Zv",
        "outputId": "6e8e10b7-e359-4f5f-9b4b-d345fffe1e35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches: [{'id': 'chunk-188',\n",
            " 'metadata': {'text': 'Ragging & Sexual Harassment •Ragging & Sexual '\n",
            "                      'Harassment of fellow students is strictly prohibited. '\n",
            "                      'Any student/s found guilty of ragging and/or abe\\x7fng '\n",
            "                      'ragging, whether ac9vely or passively, or being a part '\n",
            "                      'of a conspiracy to promote ragging, is liable to be '\n",
            "                      'punished as per the rules. Ragging ofen ends up in '\n",
            "                      'sexual or physical harassment for the vic9m. Ragging '\n",
            "                      'mostly leads to sexual abuse or harassment. •Ragging of '\n",
            "                      'students in any form is strictly prohibited inside and '\n",
            "                      'outside the campus. The'},\n",
            " 'score': 0.673096478,\n",
            " 'values': []}, {'id': 'chunk-189',\n",
            " 'metadata': {'text': 'ins9tute maintains a zero tolerance policy towards '\n",
            "                      'ragging. All issues in this regards will be dealt with '\n",
            "                      'utmost urgency and stringent ac9on will be taken '\n",
            "                      'against those involved. •Sexual harassment on campus or '\n",
            "                      'outside campus is unlawful, as well as unethical, and '\n",
            "                      'will not be tolerated. •Following is the Students’ '\n",
            "                      'Disciplinary Commiiee & Sexual Harassment Commiiee '\n",
            "                      'Students Disciplinary CommiVee •Mr. Deepak Gupta •Ms. '\n",
            "                      'Brinda Sampat •Ms. Sneha Utekar 48 People you should '\n",
            "                      'know At the University Dr.'},\n",
            " 'score': 0.615538359,\n",
            " 'values': []}]\n",
            "0.67: Ragging & Sexual Harassment •Ragging & Sexual Harassment of fellow students is strictly prohibited. Any student/s found guilty of ragging and/or abeng ragging, whether ac9vely or passively, or being a part of a conspiracy to promote ragging, is liable to be punished as per the rules. Ragging ofen ends up in sexual or physical harassment for the vic9m. Ragging mostly leads to sexual abuse or harassment. •Ragging of students in any form is strictly prohibited inside and outside the campus. The\n",
            "0.62: ins9tute maintains a zero tolerance policy towards ragging. All issues in this regards will be dealt with utmost urgency and stringent ac9on will be taken against those involved. •Sexual harassment on campus or outside campus is unlawful, as well as unethical, and will not be tolerated. •Following is the Students’ Disciplinary Commiiee & Sexual Harassment Commiiee Students Disciplinary CommiVee •Mr. Deepak Gupta •Ms. Brinda Sampat •Ms. Sneha Utekar 48 People you should know At the University Dr.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Student Support Services Guidelines\"\n",
        "\n",
        "user_query(user_question)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4XQ6kNyDCOu",
        "outputId": "559ac4e4-76d6-49ff-d954-c1f62658289f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches: [{'id': 'chunk-185',\n",
            " 'metadata': {'text': 'today has been an integral part of educa9on and is '\n",
            "                      'currently evolving to meet and exceed student '\n",
            "                      'expecta9ons. To ensure all your Queries/Concerns/Issues '\n",
            "                      'are dealt within acceptable 9meframe and to utmost '\n",
            "                      'sa9sfac9on, kindly follow the student support services '\n",
            "                      'guidelines. Policies and Procedures •Students who have '\n",
            "                      'received creden9als for Student Portal, can raise their '\n",
            "                      'queries online and will receive a request number for '\n",
            "                      'tracking purposes. •Students who are wai9ng for '\n",
            "                      '“Student Portal” access can'},\n",
            " 'score': 0.59887141,\n",
            " 'values': []}, {'id': 'chunk-184',\n",
            " 'metadata': {'text': 'cases will be reviewed by NGASCE management. A student '\n",
            "                      'shall be provided upto a maximum of 3 interviews. In '\n",
            "                      'case the student is not able to clear any of the '\n",
            "                      'interviews, the student shall be debarred from the '\n",
            "                      'placement ac9vi9es. Upon receiving a job oﬀer, the '\n",
            "                      'placement service shall conclude for that student. If '\n",
            "                      'the student has got a job oﬀer in Semester VI but has '\n",
            "                      'failed to successfully complete the program, the job '\n",
            "                      'oﬀer may be retracted. Student Support Services '\n",
            "                      'Guidelines Student services'},\n",
            " 'score': 0.571575522,\n",
            " 'values': []}, {'id': 'chunk-179',\n",
            " 'metadata': {'text': 'applicable) •Aiested copies of Grade Sheets/Mark sheets '\n",
            "                      '/ Final Cer9ﬁcate •Copy/ies of Prospectus or '\n",
            "                      'communica9on received from Professional Body/ '\n",
            "                      'Management / Educa9onal Ins9tu9on/s as applicable, '\n",
            "                      'requiring you to submit transcripts. Placement '\n",
            "                      'Guidelines: Placement assistance is oﬀered to students '\n",
            "                      'however it is the preroga9ve of the Schools & Campuses '\n",
            "                      'to decide, which of the programs this service should be '\n",
            "                      'oﬀered. Students are expected to maintain decorum and '\n",
            "                      'abide by the guidelines during'},\n",
            " 'score': 0.537595868,\n",
            " 'values': []}, {'id': 'chunk-59',\n",
            " 'metadata': {'text': 'eﬀec9ve course delivery to students enrolled in '\n",
            "                      'diﬀerent programs of NGA – SCE. However if any student '\n",
            "                      'is found sharing his login-Id & password with any '\n",
            "                      'person whether or not he/she is a student of NGA – SCE, '\n",
            "                      'he/she shall be liable to disciplinary ac9ons the '\n",
            "                      'school may think appropriate including cancella9on of '\n",
            "                      'his/her admission. 18●The access to Learning '\n",
            "                      'Resources/Contents in Student Zone shall be provided '\n",
            "                      'only for the semesters of the program for which a '\n",
            "                      'student has been enrolled and'},\n",
            " 'score': 0.482137382,\n",
            " 'values': []}, {'id': 'chunk-50',\n",
            " 'metadata': {'text': 'reading for reference •Student shall get one course '\n",
            "                      'book for each course except of Project (wherever '\n",
            "                      'applicable) •Any photocopy, scanning, pos9ng of all or '\n",
            "                      'any part of SLM on the internet is strictly prohibited. '\n",
            "                      'Any student found involved in any such ac9on jointly or '\n",
            "                      'alone may be liable to such disciplinary ac9ons as NGA '\n",
            "                      '–SCE may deem ﬁt including cancella9on of registra9on '\n",
            "                      'from the program. •Live Online Sessions •NGA – SCE '\n",
            "                      'shall conduct atleast eight hours Online live sessions '\n",
            "                      'per course, more'},\n",
            " 'score': 0.478224039,\n",
            " 'values': []}]\n",
            "0.60: today has been an integral part of educa9on and is currently evolving to meet and exceed student expecta9ons. To ensure all your Queries/Concerns/Issues are dealt within acceptable 9meframe and to utmost sa9sfac9on, kindly follow the student support services guidelines. Policies and Procedures •Students who have received creden9als for Student Portal, can raise their queries online and will receive a request number for tracking purposes. •Students who are wai9ng for “Student Portal” access can\n",
            "0.57: cases will be reviewed by NGASCE management. A student shall be provided upto a maximum of 3 interviews. In case the student is not able to clear any of the interviews, the student shall be debarred from the placement ac9vi9es. Upon receiving a job oﬀer, the placement service shall conclude for that student. If the student has got a job oﬀer in Semester VI but has failed to successfully complete the program, the job oﬀer may be retracted. Student Support Services Guidelines Student services\n",
            "0.54: applicable) •Aiested copies of Grade Sheets/Mark sheets / Final Cer9ﬁcate •Copy/ies of Prospectus or communica9on received from Professional Body/ Management / Educa9onal Ins9tu9on/s as applicable, requiring you to submit transcripts. Placement Guidelines: Placement assistance is oﬀered to students however it is the preroga9ve of the Schools & Campuses to decide, which of the programs this service should be oﬀered. Students are expected to maintain decorum and abide by the guidelines during\n",
            "0.48: eﬀec9ve course delivery to students enrolled in diﬀerent programs of NGA – SCE. However if any student is found sharing his login-Id & password with any person whether or not he/she is a student of NGA – SCE, he/she shall be liable to disciplinary ac9ons the school may think appropriate including cancella9on of his/her admission. 18●The access to Learning Resources/Contents in Student Zone shall be provided only for the semesters of the program for which a student has been enrolled and\n",
            "0.48: reading for reference •Student shall get one course book for each course except of Project (wherever applicable) •Any photocopy, scanning, pos9ng of all or any part of SLM on the internet is strictly prohibited. Any student found involved in any such ac9on jointly or alone may be liable to such disciplinary ac9ons as NGA –SCE may deem ﬁt including cancella9on of registra9on from the program. •Live Online Sessions •NGA – SCE shall conduct atleast eight hours Online live sessions per course, more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Placement Guidelines\"\n",
        "\n",
        "user_query(user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7xyeuspDmio",
        "outputId": "03e533ba-9ffe-41f1-e554-ed91778503c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top matches: [{'id': 'chunk-179',\n",
            " 'metadata': {'text': 'applicable) •Aiested copies of Grade Sheets/Mark sheets '\n",
            "                      '/ Final Cer9ﬁcate •Copy/ies of Prospectus or '\n",
            "                      'communica9on received from Professional Body/ '\n",
            "                      'Management / Educa9onal Ins9tu9on/s as applicable, '\n",
            "                      'requiring you to submit transcripts. Placement '\n",
            "                      'Guidelines: Placement assistance is oﬀered to students '\n",
            "                      'however it is the preroga9ve of the Schools & Campuses '\n",
            "                      'to decide, which of the programs this service should be '\n",
            "                      'oﬀered. Students are expected to maintain decorum and '\n",
            "                      'abide by the guidelines during'},\n",
            " 'score': 0.610876501,\n",
            " 'values': []}, {'id': 'chunk-180',\n",
            " 'metadata': {'text': 'placement processes. In the event of non-conformance to '\n",
            "                      'the placement guidelines, the School reserves the right '\n",
            "                      'to ini9ate correc9ve ac9on. In addi9on to the '\n",
            "                      'Employability Skills Module NGASCE will oﬀer Placement '\n",
            "                      'support to students Employability Skills: Module: Sof '\n",
            "                      'Skills for Managers Skills: Sof Skills Hours: 10 '\n",
            "                      'Semester: I Module: Excel with MS Excel Skills: '\n",
            "                      'Technical Hours: 10 Semester: II Module: Start your '\n",
            "                      'Start up Skills: Technical Hours: 10 Semester: III '\n",
            "                      'Content: Curated content Online'},\n",
            " 'score': 0.489796042,\n",
            " 'values': []}, {'id': 'chunk-181',\n",
            " 'metadata': {'text': 'Sessions – The Lectures for Employability Skills will '\n",
            "                      'be conducted on weekends Assessment: Faculty driven '\n",
            "                      'grading of 30 Marks for each Module (may be in the form '\n",
            "                      'of Assignment) Note – The marks for Employability '\n",
            "                      'Skills will not be included in the Marksheet. '\n",
            "                      'Eligibility Criteria for registering for Placement '\n",
            "                      'Ac;vi;es: Course Comple9on Criteria: 90% of courses 9ll '\n",
            "                      'Semester IV should be successfully completed at the 9me '\n",
            "                      'of registra9on for placement ac9vi9es (in Semester V). '\n",
            "                      '46100% of courses 9ll'},\n",
            " 'score': 0.436870575,\n",
            " 'values': []}, {'id': 'chunk-182',\n",
            " 'metadata': {'text': 'Semester V should be successfully completed at the 9me '\n",
            "                      'of commencement of Job Search (in Semester VI). Marks '\n",
            "                      'Criteria: Minimum 60% required at the 9me of '\n",
            "                      'registra9on for placements (Aggregate of Term I to IV) '\n",
            "                      'in Semester V. Minimum 60% required in each '\n",
            "                      'Employability skills module at the 9me of commencement '\n",
            "                      'of Job Search (in Semester VI). Placement Guidelines '\n",
            "                      'Registra9on for placement ac9vi9es will happen at the '\n",
            "                      '9me of Semester V registra9on. Fee for placement '\n",
            "                      'ac9vi9es need to be paid at the 9me'},\n",
            " 'score': 0.425006926,\n",
            " 'values': []}, {'id': 'chunk-183',\n",
            " 'metadata': {'text': 'of registra9on. Placement ac9vi9es will be con9ngent to '\n",
            "                      'student comple9ng the BBA / B.Com Program. Student '\n",
            "                      'should have successfully completed all employability '\n",
            "                      'skills modules at the 9me of registra9on for placement '\n",
            "                      'ac9vi9es. Condi;ons for Job Search Ac;vity: Students '\n",
            "                      'shall be provided placement within top 5 industries and '\n",
            "                      'loca9ons chosen during Career Counselling session. It '\n",
            "                      'is mandatory for a student to aiend a job interview, '\n",
            "                      'failing which no further opportuni9es shall be '\n",
            "                      'provided. Excep9onal'},\n",
            " 'score': 0.422098309,\n",
            " 'values': []}]\n",
            "0.61: applicable) •Aiested copies of Grade Sheets/Mark sheets / Final Cer9ﬁcate •Copy/ies of Prospectus or communica9on received from Professional Body/ Management / Educa9onal Ins9tu9on/s as applicable, requiring you to submit transcripts. Placement Guidelines: Placement assistance is oﬀered to students however it is the preroga9ve of the Schools & Campuses to decide, which of the programs this service should be oﬀered. Students are expected to maintain decorum and abide by the guidelines during\n",
            "0.49: placement processes. In the event of non-conformance to the placement guidelines, the School reserves the right to ini9ate correc9ve ac9on. In addi9on to the Employability Skills Module NGASCE will oﬀer Placement support to students Employability Skills: Module: Sof Skills for Managers Skills: Sof Skills Hours: 10 Semester: I Module: Excel with MS Excel Skills: Technical Hours: 10 Semester: II Module: Start your Start up Skills: Technical Hours: 10 Semester: III Content: Curated content Online\n",
            "0.44: Sessions – The Lectures for Employability Skills will be conducted on weekends Assessment: Faculty driven grading of 30 Marks for each Module (may be in the form of Assignment) Note – The marks for Employability Skills will not be included in the Marksheet. Eligibility Criteria for registering for Placement Ac;vi;es: Course Comple9on Criteria: 90% of courses 9ll Semester IV should be successfully completed at the 9me of registra9on for placement ac9vi9es (in Semester V). 46100% of courses 9ll\n",
            "0.43: Semester V should be successfully completed at the 9me of commencement of Job Search (in Semester VI). Marks Criteria: Minimum 60% required at the 9me of registra9on for placements (Aggregate of Term I to IV) in Semester V. Minimum 60% required in each Employability skills module at the 9me of commencement of Job Search (in Semester VI). Placement Guidelines Registra9on for placement ac9vi9es will happen at the 9me of Semester V registra9on. Fee for placement ac9vi9es need to be paid at the 9me\n",
            "0.42: of registra9on. Placement ac9vi9es will be con9ngent to student comple9ng the BBA / B.Com Program. Student should have successfully completed all employability skills modules at the 9me of registra9on for placement ac9vi9es. Condi;ons for Job Search Ac;vity: Students shall be provided placement within top 5 industries and loca9ons chosen during Career Counselling session. It is mandatory for a student to aiend a job interview, failing which no further opportuni9es shall be provided. Excep9onal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are unlikely to be in the uploaded pdf\n",
        "\"\"\"\n",
        "user_question = \"Are we alone in this Universe?\"\n",
        "user_query(user_question)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghIn4shCR5bh",
        "outputId": "d835622a-8f9c-41d1-941c-72ce5d1c72a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Apport Software Solutions Private Limited\"\n",
        "user_query(user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9468ZkNDvKt",
        "outputId": "16a0654f-f023-4e96-de27-b0c478db201b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"Seating Plan\"\n",
        "user_query(user_question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiQ6oKybEFgo",
        "outputId": "31c9db03-c3e8-4eeb-dba1-1c32d9b58c5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gyuY3quYOXWp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using \"facebook/bart-large-cnn\" for better summarization"
      ],
      "metadata": {
        "id": "PDVieI44G7MR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Piping and Instrumentation Diagram](https://drive.google.com/uc?id=11qtF7oPjv5YzeMCgUH9QnhzlannWglFD)\n",
        "\n"
      ],
      "metadata": {
        "id": "gxWMEjO3OYZJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Facebook BART Architecture\n",
        "\n",
        "BART (Bidirectional and Auto-Regressive Transformers) is a sequence-to-sequence model that combines the strengths of BERT's bidirectional encoder and GPT's autoregressive decoder. It is trained using a denoising autoencoder approach, where the model learns to reconstruct corrupted text. BART excels at tasks like text summarization, generation, and translation due to its ability to both understand and generate text. Its hybrid architecture allows it to perform well on a variety of natural language processing tasks, making it a versatile and powerful tool in NLP.\n",
        "\n",
        "![Sentence Transformer Architecture](https://production-media.paperswithcode.com/methods/Screen_Shot_2020-06-01_at_9.49.47_PM.png)\n",
        "\n",
        " [BART Docs](https://huggingface.co/docs/transformers/en/model_doc/bart)."
      ],
      "metadata": {
        "id": "aeonMgk6TC51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whD5kSegG7mk",
        "outputId": "d1d7522f-efd0-4084-9571-410efb6d3db3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fallback Response and Concise Answer Generation\n",
        "\n",
        "#### Purpose:\n",
        "This approach ensures that the chatbot always provides a response, even when no relevant matches are found for the user's query.\n",
        "\n",
        "#### How It Works:\n",
        "1. **Fallback Response**: If the chatbot doesn't find any valid matches with a high enough similarity score, it returns a predefined fallback response:  \n",
        "   *“Sorry, I didn’t understand your question. Do you want to connect with a live agent?”*\n",
        "\n",
        "2. **Concise Response**: If valid matches are found (i.e., matches with a score above the specified threshold), the chatbot combines the matching text, uses a summarization model (e.g., BART or T5) to generate a concise, user-friendly response, and presents it to the user.\n",
        "\n",
        "#### Why It Is Used:\n",
        "- **Ensures User Satisfaction**: The fallback response ensures the chatbot remains functional even when it cannot find a relevant answer, preventing an empty or irrelevant response.\n",
        "- **Natural and Concise**: The summarization step converts lengthy or complex matches into concise, easy-to-read answers, improving the user experience.\n",
        "- **Smooth Transition**: If the chatbot can't answer the query, the fallback message offers a smooth transition to a live agent, ensuring continuous support.\n"
      ],
      "metadata": {
        "id": "tRdVBTqBKOOi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Piping and Instrumentation Diagram](https://drive.google.com/uc?id=1A-Y2fCpNg9gwFDn4RnXdqQLw2_AzakPC)\n",
        "\n",
        "Overall Design\n",
        "\n",
        "**Correction** : Result--> INTO --> Summarizer (BART) will also be in backend"
      ],
      "metadata": {
        "id": "clfbp3RROp5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load pre-trained model for summarization (BART or T5)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "1Yhq2Q2PHlZx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_concise_response(query_text, matches, similarity_threshold=0.4):\n",
        "    \"\"\"\n",
        "    Generate a concise and user-friendly response by rephrasing the matched text.\n",
        "    If no valid matches are found, returns a fallback response.\n",
        "    Args:\n",
        "        query_text (str): The user's query.\n",
        "        matches (list): List of top matches with metadata.\n",
        "        similarity_threshold (float): Minimum similarity score to consider a match valid.\n",
        "    Returns:\n",
        "        str: Concise response based on matched text, or fallback response.\n",
        "    \"\"\"\n",
        "\n",
        "    valid_matches = [match for match in matches if match[\"score\"] >= similarity_threshold]\n",
        "\n",
        "    if not valid_matches:\n",
        "        # fallback response\n",
        "        return \"Sorry, I didn’t understand your question. Do you want to connect with a live agent?\"\n",
        "\n",
        "\n",
        "    combined_text = \" \".join([match['metadata']['text'] for match in valid_matches])\n",
        "\n",
        "\n",
        "    summarized_response = summarizer(combined_text, max_length=200, min_length=50, do_sample=False)\n",
        "\n",
        "    return summarized_response[0]['summary_text']\n"
      ],
      "metadata": {
        "id": "P-HSaoCcHiBF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Use Cases"
      ],
      "metadata": {
        "id": "SMrL1HVgJRm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are the consequence of ragging\"\n",
        "results = query_pinecone(user_query)\n",
        "\n",
        "\n",
        "concise_response = generate_concise_response(user_query, results)\n",
        "\n",
        "print(concise_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tfw_P74H6cr",
        "outputId": "2f190577-7175-4c6b-8ea7-462f1f704c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ragging of students in any form is strictly prohibited inside and outside the campus. Ragging ofen ends up in sexual or physical harassment for the vic9m. The ins9tute maintains a zero tolerance policy towards ragging. All issues in this regards will be dealt with utmost urgency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"what are Term End Examinaon Eligibility & Policies\"\n",
        "results = query_pinecone(user_query)\n",
        "\n",
        "\n",
        "concise_response = generate_concise_response(user_query, results)\n",
        "\n",
        "print(concise_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "755HDkiVJY7G",
        "outputId": "a69be2c0-7a60-4562-a03f-bc197ea25533"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Term End Examina9on Credence is 70%. Students are expected to complete the academic cycle of the Semester enrolled for. Students cannot directly appear for Re-Sit Term End Exams (April/Sept) Students can register directly for the term end examina9on based on the eligibility.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Fallback Responses"
      ],
      "metadata": {
        "id": "P89Cf0e8JoFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"Do Aliens exists ?\"\n",
        "results = query_pinecone(user_query)\n",
        "\n",
        "\n",
        "concise_response = generate_concise_response(user_query, results)\n",
        "\n",
        "print(concise_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zn7_ybPUJvO6",
        "outputId": "476c8626-b3b3-41b2-a89f-40109881a9cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What are ASR Models\"\n",
        "results = query_pinecone(user_query)\n",
        "\n",
        "\n",
        "concise_response = generate_concise_response(user_query, results)\n",
        "\n",
        "print(concise_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V95tD90CJ3Zj",
        "outputId": "c0739bb2-0d48-40b4-9c3c-9821be9f1b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T99N40ZgSOrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fallback Logic"
      ],
      "metadata": {
        "id": "Sb8516NfSD1E"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ItsvY1lej6dz"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling Fallback Responses and Threshold Optimization\n",
        "\n",
        "#### 1. **Threshold for Similarity Score**\n",
        "\n",
        "The threshold for similarity scores plays a crucial role in determining when the bot should respond with relevant information and when it should trigger a fallback response.\n",
        "\n",
        "- **Precision** ensures that only highly relevant matches are considered.\n",
        "- **Recall** ensures that potential relevant information isn’t overlooked.\n",
        "\n",
        "**Threshold Optimization:**\n",
        "\n",
        "- **High Threshold (e.g., 0.9)**: May result in many fallback responses, leading to user frustration.\n",
        "- **Low Threshold (e.g., 0.3)**: Can cause irrelevant or weak responses, impacting user satisfaction.\n",
        "\n",
        "**Optimal Threshold Selection**:\n",
        "- Start with a **reasonable threshold** (e.g., 0.6).\n",
        "- **Iterate and experiment** based on feedback or test cases.\n",
        "- Adjust based on **domain knowledge** of the PDF content.\n",
        "\n",
        "#### 2. **Graceful Fallback Handling**\n",
        "\n",
        "Fallback responses should be **empathetic** and **context-aware** to maintain user engagement.\n",
        "\n",
        "- **Tone**: Ensure responses are friendly and supportive, e.g.,  \n",
        "  *“I couldn’t find an exact match. Would you like to ask something else or speak to a live agent?”*\n",
        "  \n",
        "- **Clarification**: If unable to provide an exact match, clarify the limitation, e.g.,  \n",
        "  *“Your query seems a bit specific. Would you like to rephrase or try another question?”*\n",
        "\n",
        "- **Suggestions**: Propose related topics or areas of interest to keep the conversation flowing, e.g.,  \n",
        "  *“Do you mean placement guidelines or eligibility criteria?”*\n",
        "\n",
        "#### 3. **Confidence Level Adjustment**\n",
        "\n",
        "- **Dynamic Threshold Adjustment**: Adjust the threshold based on the confidence of the match. For **lower-confidence matches** (e.g., scores between 0.6-0.7), consider a more flexible fallback response:  \n",
        "  *“I found some information but it may not be what you’re looking for. Would you like me to try again?”*\n",
        "  \n",
        "- **Higher Confidence** (e.g., scores above 0.8) should result in more confident, concise responses.\n",
        "\n",
        "#### 4. **Best Practices**\n",
        "\n",
        "- **Monitor and Fine-Tune**: Regularly track fallback response frequency and adjust the threshold as needed.\n",
        "  \n",
        "- **Context-Awareness**: Maintain context for follow-up queries, especially when exact matches are not found.\n",
        "\n",
        "- **Aggregated Responses**: For queries with multiple matches, aggregate top results and rephrase them concisely for a better response.\n",
        "\n",
        "\n",
        "#### Conclusion:\n",
        "- Fine-tuning the similarity threshold is a balance between **precision** and **recall**, with **user feedback** guiding adjustments.\n",
        "- **Fallback responses** should be empathetic and provide options for re-engagement, ensuring a smooth and professional user experience.\n"
      ],
      "metadata": {
        "id": "ILUYT3KySLza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sources\n",
        "- [Learn about Vector Databases (Pinecone)](https://www.pinecone.io/learn/vector-database/)\n",
        "- [Sentence Transformers (SBERT)](https://sbert.net/)\n",
        "- [New and Improved Embedding Model (OpenAI)](https://openai.com/index/new-and-improved-embedding-model/)\n",
        "- [Colab Notebook Example](https://colab.research.google.com/drive/13W-X7MPBSBww1eNnhCHS1x5U6yqptFPi?usp=sharing)\n"
      ],
      "metadata": {
        "id": "YcH7cJItyhBO"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0ac59b138acc49629a97d83cbad9b9f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "model_module_version": "1.5.0",
          "state": {
            "_counter": 1,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": ".pdf",
            "button_style": "",
            "data": [
              null
            ],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_74207a5c63644fc4a2dda0ae5fc47bd6",
            "metadata": [
              {
                "name": "input.pdf",
                "type": "application/pdf",
                "size": 824028,
                "lastModified": 1732935222879
              }
            ],
            "multiple": false,
            "style": "IPY_MODEL_269d975266a3420daafc85b97d1f2dcd"
          }
        },
        "74207a5c63644fc4a2dda0ae5fc47bd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "269d975266a3420daafc85b97d1f2dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "66d6bcb1fbb841c68123010f191cb7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc8a4cda10c54953be061c805b25b310",
              "IPY_MODEL_fdd6beac6c97472c98b3fad10d4d17f5",
              "IPY_MODEL_432837570eba48c380e2ca147e732488"
            ],
            "layout": "IPY_MODEL_f7bd204320f04fed971431627fe424de"
          }
        },
        "dc8a4cda10c54953be061c805b25b310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efd7e1fb0cfc4a368a14c257c46c374a",
            "placeholder": "​",
            "style": "IPY_MODEL_52da5e6c579944b4b3f06e4aab8b788b",
            "value": "Batches: 100%"
          }
        },
        "fdd6beac6c97472c98b3fad10d4d17f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32e0bac87f06454a824e4bbae6f008de",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b889b76618a42d6b944e738bc8f4c1c",
            "value": 6
          }
        },
        "432837570eba48c380e2ca147e732488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_311f1a5011e442da802574706b1fd973",
            "placeholder": "​",
            "style": "IPY_MODEL_a4d08f06438c4d6389096e9d3dfdab53",
            "value": " 6/6 [00:17&lt;00:00,  2.74s/it]"
          }
        },
        "f7bd204320f04fed971431627fe424de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd7e1fb0cfc4a368a14c257c46c374a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52da5e6c579944b4b3f06e4aab8b788b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32e0bac87f06454a824e4bbae6f008de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b889b76618a42d6b944e738bc8f4c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "311f1a5011e442da802574706b1fd973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d08f06438c4d6389096e9d3dfdab53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}